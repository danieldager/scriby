{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7eec0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transcribe.py\n",
    "from faster_whisper import WhisperModel\n",
    "from pathlib import Path\n",
    "import torch, argparse, math\n",
    "\n",
    "def ts(sec: float) -> str:\n",
    "    \"\"\"seconds → SRT timestamp (HH:MM:SS,mmm)\"\"\"\n",
    "    h, sec = divmod(sec, 3600)\n",
    "    m, sec = divmod(sec, 60)\n",
    "    return f\"{int(h):02}:{int(m):02}:{int(sec):02},{int((sec-int(sec))*1000):03}\"\n",
    "\n",
    "def transcribe(audio, model_size=\"large-v3\", lang=\"fr\"):\n",
    "    model = WhisperModel(model_size,\n",
    "                         device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "                         compute_type=\"auto\")  # quantised = less VRAM\n",
    "    segments, _ = model.transcribe(audio, language=lang)\n",
    "    \n",
    "    base = Path(audio).with_suffix(\"\")\n",
    "    text_out, srt_out = [], []\n",
    "\n",
    "    for i, segment in enumerate(segments):\n",
    "        start = segment.start\n",
    "        end   = segment.end\n",
    "        txt   = segment.text\n",
    "        text_out.append(txt)\n",
    "        srt_out.append(f\"{i+1}\\n{ts(start)} --> {ts(end)}\\n{txt.strip()}\\n\")\n",
    "    \n",
    "    base.with_suffix(\".txt\").write_text(\"\\n\".join(text_out), encoding=\"utf-8\")\n",
    "    base.with_suffix(\".srt\").write_text(\"\\n\".join(srt_out), encoding=\"utf-8\")\n",
    "    print(f\"✓ Wrote {base}.txt and {base}.srt\")\n",
    "\n",
    "transcribe(\"data/sample.mp4\", \"large-v3\", \"fr\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     ap = argparse.ArgumentParser()\n",
    "#     ap.add_argument(\"audio\", help=\"audio/video file (wav, mp3, mp4, etc.)\")\n",
    "#     ap.add_argument(\"--model\", default=\"large-v3\", help=\"tiny|base|small|...\"\n",
    "#                     \" or large-v3 for best French accuracy\")\n",
    "#     args = ap.parse_args()\n",
    "#     transcribe(args.audio, args.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c759ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transcribe.py\n",
    "from faster_whisper import WhisperModel\n",
    "from pathlib import Path\n",
    "\n",
    "def ts(sec: float) -> str:\n",
    "    \"\"\"seconds → SRT timestamp (HH:MM:SS,mmm)\"\"\"\n",
    "    h, sec = divmod(sec, 3600)\n",
    "    m, sec = divmod(sec, 60)\n",
    "    return f\"{int(h):02}:{int(m):02}:{int(sec):02},{int((sec-int(sec))*1000):03}\"\n",
    "\n",
    "def transcribe(audio, model_size=\"large-v3\", lang=\"fr\"):\n",
    "    model = WhisperModel(model_size,\n",
    "                         device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "                         compute_type=\"auto\")  # quantised = less VRAM\n",
    "    segments, _ = model.transcribe(audio, language=lang)\n",
    "    \n",
    "    base = Path(audio).with_suffix(\"\")\n",
    "    text_out, srt_out = [], []\n",
    "\n",
    "    for i, segment in enumerate(segments):\n",
    "        start = segment.start\n",
    "        end   = segment.end\n",
    "        txt   = segment.text\n",
    "        text_out.append(txt)\n",
    "        srt_out.append(f\"{i+1}\\n{ts(start)} --> {ts(end)}\\n{txt.strip()}\\n\")\n",
    "    \n",
    "    base.with_suffix(\".txt\").write_text(\"\\n\".join(text_out), encoding=\"utf-8\")\n",
    "    base.with_suffix(\".srt\").write_text(\"\\n\".join(srt_out), encoding=\"utf-8\")\n",
    "    print(f\"✓ Wrote {base}.txt and {base}.srt\")\n",
    "\n",
    "transcribe(\"data/sample.mp4\", \"large-v3\", \"fr\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
